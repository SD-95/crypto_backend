{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d162b4c2",
   "metadata": {},
   "source": [
    "# Cryptocurrency Liquidity Prediction for Market Stability\n",
    "\n",
    "**Life cycle of Machine learning Project**\n",
    "\n",
    "`Part-2`the file contains below key information\n",
    "\n",
    "* Data Splitting (Train_test_split)\n",
    "* Model Selection\n",
    "* Model Training\n",
    "* Hyperparameter Tuning\n",
    "* Model Testing & Validation\n",
    "* Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0f8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                       # for Data mannupulation\n",
    "import numpy as np                        # numerical python libraries, for scientifical calculation of the data\n",
    "import matplotlib.pyplot as plt           # data visualtion library\n",
    "import datetime as dt                     # Time series data library\n",
    "import seaborn as sns                     # data visualtion library\n",
    "\n",
    "import warnings                           # some library have some warning messages, \n",
    "warnings.filterwarnings(\"ignore\")         # if we ignore it thoese warning message will not show in the notebook. the notebook looks better\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416b24b",
   "metadata": {},
   "source": [
    "## 6) Data Splitting (Train_test_split)\n",
    "\n",
    "* it is based on cryptocurrency liquidity prediction using time-series data, it’s crucial to split the dataset in a way that respects time order.\n",
    "* In time series, each row depends on the past — so we cannot randomly shuffle and split the data like in normal datasets.\n",
    "* We must train on the past and test on the future to simulate real-world prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e182d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('final_df.csv') # read the final csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc930c32",
   "metadata": {},
   "source": [
    "### Splitting the independent variable(x) and target variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876f0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define feature columns and target\n",
    "features = ['1h', '24h', '7d', 'price_lag1', 'volume_lag1', 'mktcap_lag1',\n",
    "            'price_2d_avg', 'volume_2d_avg', 'vol_to_mcap', 'vol_price_ratio']\n",
    "\n",
    "x = final_df[features]\n",
    "y = final_df['liquidity_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a311f",
   "metadata": {},
   "source": [
    "### Encode the target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65704d71",
   "metadata": {},
   "source": [
    "### Label Encoding and One-Hot Conversion\n",
    "\n",
    "Before feeding the labels into our machine learning and deep learning models, we need to convert them from categorical strings into a numerical format.\n",
    "\n",
    "- `LabelEncoder` converts string labels like 'low', 'medium', 'high' into integers: 0, 1, 2.\n",
    "- `to_categorical` then converts these integers into one-hot encoded vectors, which are required for classification using LSTM models.\n",
    "\n",
    "This ensures compatibility with classification layers like `Dense(3, activation='softmax')` and the `categorical_crossentropy` loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268983e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # low=0, medium=1, high=2\n",
    "y_categorical = to_categorical(y_encoded)  # for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78632319",
   "metadata": {},
   "source": [
    "- This function is preparing your data for use in sequence models like LSTM — which require input in the form of sequences over time.\n",
    "- In time series or temporal modeling (e.g., predicting cryptocurrency liquidity), we often want to use past n steps (e.g., past 2 days' features) to predict the next value (e.g., tomorrow's liquidity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85268e8",
   "metadata": {},
   "source": [
    "### Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88765d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b976e8",
   "metadata": {},
   "source": [
    "### Create sequence data for LSTM\n",
    "\n",
    "LSTM models are designed to learn from sequential or time-dependent data. To use LSTM, we need to structure our features into **sequences**.\n",
    "\n",
    "#### 🔧 `create_sequences()` Function\n",
    "This function takes:\n",
    "- `data`: Scaled input features (`X_scaled`)\n",
    "- `target`: Encoded output labels (`y_encoded`)\n",
    "- `time_step`: The number of previous time steps to use in each input sequence\n",
    "\n",
    "It returns:\n",
    "- `X_seq`: A 3D array shaped as `(samples, time_steps, features)`\n",
    "- `y_seq`: Corresponding target values for each sequence\n",
    "\n",
    "#### Example:\n",
    "If `time_step = 1`:\n",
    "- Each sample contains 1 row of features → shape: `(n_samples, 1, n_features)`\n",
    "- Each target corresponds to the label **after** the time step.\n",
    "\n",
    "#### ✅ Why this is important:\n",
    "- LSTM models require 3D input: `[samples, time_steps, features]`\n",
    "- This transformation lets the LSTM learn from **patterns across time steps**, even if `time_step=1`.\n",
    "\n",
    "You can increase `time_step` to look further back in time when predicting future values, especially if your data is time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7688db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq shape: (999, 1, 10)\n",
      "y_seq shape: (999,)\n"
     ]
    }
   ],
   "source": [
    "# 3. Create sequences\n",
    "def create_sequences(data, target, time_step=1):        # data: a 2D NumPy array or DataFrame (e.g., your scaled features like price, volume, etc.)\n",
    "                                                        # target: a 1D array (e.g., liquidity values you want to predict)\n",
    "                                                        # time_step: number of previous time steps to use for predicting the next one (default = 2)\n",
    "    xs, ys = [], []                                     # Xs: will store input sequences (each sequence is a block of time_step rows of features)\n",
    "                                                        # ys: will store the corresponding target value (the value right after each sequence)\n",
    "    for i in range(len(data) - time_step):              # Loops from i = 0 to len(data) - time_step - 1\n",
    "        xs.append(data[i:(i+time_step)])                # Appends a sequence of time_step rows starting at row i\n",
    "        ys.append(target[i + time_step])                # Appends the target value that comes after the current input sequence\n",
    "    return np.array(xs), np.array(ys)                   # Converts the lists Xs and ys into NumPy arrays\n",
    "\n",
    "\n",
    "# Call the function to create sequences\n",
    "x_seq, y_seq = create_sequences(x_scaled, y_encoded, time_step=1) # X_scaled: your features (2D)\n",
    "                                                                 # y_encoded: encoded values\n",
    "                                                                 # time_step=1\n",
    "\n",
    "\n",
    "print(\"X_seq shape:\", x_seq.shape)  # (samples, time_steps, features)\n",
    "print(\"y_seq shape:\", y_seq.shape)  # (samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78513ff5",
   "metadata": {},
   "source": [
    "### Train_test_split step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d520b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_seq, y_seq, test_size=0.2, random_state=42, shuffle=False  # Important: shuffle=False for time series\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c3e23",
   "metadata": {},
   "source": [
    "Checking the shape of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f3ffd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((799, 1, 10), (799,), (200, 1, 10), (200,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42addc",
   "metadata": {},
   "source": [
    "### Flattened features for Random Forest\n",
    "It flattens each time-series sequence in X_train and X_test into a 1D vector, while keeping the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea55725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f1b8e",
   "metadata": {},
   "source": [
    "## 7) model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af2f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441ae28",
   "metadata": {},
   "source": [
    "## 8) model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d305cf",
   "metadata": {},
   "source": [
    "### Random Forest Classifier `model-1` - Base Model\n",
    "\n",
    "We use a `RandomForestClassifier` as one of the base models in our ensemble (stacking) architecture.\n",
    "\n",
    "```python\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(x_train_flat, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e22230",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier()\n",
    "random_forest_clf.fit(x_train_flat, y_train)\n",
    "rf_preds = random_forest_clf.predict(x_test_flat)\n",
    "rf_preds_proba = random_forest_clf.predict_proba(x_test_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed7d40",
   "metadata": {},
   "source": [
    "### LSTM Classifier for Multiclass Classification `model-2`\n",
    "\n",
    "We define an LSTM-based neural network to classify each data point into one of three **liquidity levels**: `low`, `medium`, or `high`.\n",
    "\n",
    "#### Convert y_train to categorical for LSTM training\n",
    "\n",
    "We now train our LSTM model to predict the liquidity level class (`low`, `medium`, `high`) based on input features.\n",
    "\n",
    "#### One-Hot Encoding of Target Labels\n",
    "```python\n",
    "y_train_categorical = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c64d1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23/23 - 5s - 196ms/step - accuracy: 0.5118 - loss: 1.0890 - val_accuracy: 0.5500 - val_loss: 1.0824\n",
      "Epoch 2/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5327 - loss: 1.0691 - val_accuracy: 0.5375 - val_loss: 1.0642\n",
      "Epoch 3/30\n",
      "23/23 - 0s - 8ms/step - accuracy: 0.5341 - loss: 1.0399 - val_accuracy: 0.5375 - val_loss: 1.0397\n",
      "Epoch 4/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5285 - loss: 1.0048 - val_accuracy: 0.5250 - val_loss: 1.0197\n",
      "Epoch 5/30\n",
      "23/23 - 0s - 8ms/step - accuracy: 0.5271 - loss: 0.9784 - val_accuracy: 0.5250 - val_loss: 1.0103\n",
      "Epoch 6/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5438 - loss: 0.9615 - val_accuracy: 0.5375 - val_loss: 1.0087\n",
      "Epoch 7/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5466 - loss: 0.9540 - val_accuracy: 0.5500 - val_loss: 0.9996\n",
      "Epoch 8/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5633 - loss: 0.9437 - val_accuracy: 0.5625 - val_loss: 0.9971\n",
      "Epoch 9/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5647 - loss: 0.9365 - val_accuracy: 0.5500 - val_loss: 0.9907\n",
      "Epoch 10/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.5800 - loss: 0.9286 - val_accuracy: 0.5875 - val_loss: 0.9880\n",
      "Epoch 11/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6120 - loss: 0.9202 - val_accuracy: 0.6000 - val_loss: 0.9825\n",
      "Epoch 12/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6106 - loss: 0.9123 - val_accuracy: 0.5875 - val_loss: 0.9790\n",
      "Epoch 13/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6147 - loss: 0.9050 - val_accuracy: 0.5750 - val_loss: 0.9762\n",
      "Epoch 14/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6134 - loss: 0.8970 - val_accuracy: 0.5625 - val_loss: 0.9756\n",
      "Epoch 15/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6022 - loss: 0.8896 - val_accuracy: 0.5625 - val_loss: 0.9694\n",
      "Epoch 16/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6022 - loss: 0.8823 - val_accuracy: 0.5625 - val_loss: 0.9691\n",
      "Epoch 17/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6078 - loss: 0.8758 - val_accuracy: 0.5625 - val_loss: 0.9662\n",
      "Epoch 18/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6050 - loss: 0.8696 - val_accuracy: 0.5625 - val_loss: 0.9613\n",
      "Epoch 19/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6078 - loss: 0.8623 - val_accuracy: 0.5625 - val_loss: 0.9610\n",
      "Epoch 20/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6022 - loss: 0.8564 - val_accuracy: 0.5625 - val_loss: 0.9542\n",
      "Epoch 21/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6064 - loss: 0.8510 - val_accuracy: 0.5750 - val_loss: 0.9525\n",
      "Epoch 22/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6036 - loss: 0.8471 - val_accuracy: 0.5625 - val_loss: 0.9545\n",
      "Epoch 23/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6134 - loss: 0.8413 - val_accuracy: 0.5750 - val_loss: 0.9513\n",
      "Epoch 24/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6092 - loss: 0.8369 - val_accuracy: 0.5750 - val_loss: 0.9622\n",
      "Epoch 25/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6134 - loss: 0.8317 - val_accuracy: 0.5750 - val_loss: 0.9528\n",
      "Epoch 26/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6120 - loss: 0.8285 - val_accuracy: 0.5625 - val_loss: 0.9489\n",
      "Epoch 27/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6078 - loss: 0.8237 - val_accuracy: 0.5625 - val_loss: 0.9575\n",
      "Epoch 28/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6161 - loss: 0.8198 - val_accuracy: 0.5750 - val_loss: 0.9582\n",
      "Epoch 29/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6147 - loss: 0.8158 - val_accuracy: 0.5875 - val_loss: 0.9587\n",
      "Epoch 30/30\n",
      "23/23 - 0s - 9ms/step - accuracy: 0.6147 - loss: 0.8139 - val_accuracy: 0.5875 - val_loss: 0.9641\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "lstm_model.add(LSTM(32))\n",
    "lstm_model.add(Dense(3, activation='softmax'))  # 3 classes\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "\n",
    "lstm_model.fit(x_train, y_train_categorical, epochs=30, batch_size=32, validation_split=0.1, verbose=2)\n",
    "lstm_preds_proba = lstm_model.predict(x_test)\n",
    "lstm_preds = np.argmax(lstm_preds_proba, axis=1)  # predicted class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80967146",
   "metadata": {},
   "source": [
    " ### 🔄 Building the Meta-Model for Stacking Ensemble (Logistic Regression on stacked probabilities)\n",
    "\n",
    "We now stack the predictions of the Random Forest and LSTM models to train a **meta-classifier** (Logistic Regression). This final model aims to combine the strengths of both base models.\n",
    "\n",
    "####  Combine Base Model Outputs\n",
    "\n",
    "meta_features_test = np.hstack((rf_preds_proba, lstm_preds_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8317bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features_test = np.hstack((rf_preds_proba, lstm_preds_proba))\n",
    "meta_model = LogisticRegression(max_iter=200)\n",
    "meta_model.fit(meta_features_test, y_test)  # Note: Ideally train on validation set, not test\n",
    "meta_preds = meta_model.predict(meta_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f596b6",
   "metadata": {},
   "source": [
    "## 9) hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6776f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "Best RF Parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best RF CV Score: 0.612781954887218\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                       param_grid=rf_param_grid, cv=tscv,\n",
    "                       scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_rf.fit(x_train_flat, y_train)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "rf_preds_proba = best_rf.predict_proba(x_test_flat)\n",
    "\n",
    "print(\"Best RF Parameters:\", grid_rf.best_params_)\n",
    "print(\"Best RF CV Score:\", grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50610c1",
   "metadata": {},
   "source": [
    "## 10) Model Testing & Validation (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b551e6c",
   "metadata": {},
   "source": [
    "### Final model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37eca387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.565\n",
      "LSTM Accuracy: 0.545\n",
      "Meta Model Accuracy: 0.575\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.58      0.60      0.59        68\n",
      "         low       0.54      0.57      0.56        56\n",
      "      medium       0.57      0.53      0.55        76\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.56      0.57      0.56       200\n",
      "weighted avg       0.57      0.56      0.56       200\n",
      "\n",
      "\n",
      "Classification Report (LSTM):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.62      0.59      0.61        68\n",
      "         low       0.49      0.59      0.54        56\n",
      "      medium       0.52      0.47      0.50        76\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.55      0.55      0.55       200\n",
      "weighted avg       0.55      0.55      0.54       200\n",
      "\n",
      "\n",
      "Classification Report (Meta Model):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        high       0.62      0.60      0.61        68\n",
      "         low       0.60      0.50      0.54        56\n",
      "      medium       0.53      0.61      0.56        76\n",
      "\n",
      "    accuracy                           0.57       200\n",
      "   macro avg       0.58      0.57      0.57       200\n",
      "weighted avg       0.58      0.57      0.57       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_preds))\n",
    "print(\"LSTM Accuracy:\", accuracy_score(y_test, lstm_preds))\n",
    "print(\"Meta Model Accuracy:\", accuracy_score(y_test, meta_preds))\n",
    "\n",
    "print(\"\\nClassification Report (Random Forest):\\n\", classification_report(y_test, rf_preds, target_names=le.classes_))\n",
    "print(\"\\nClassification Report (LSTM):\\n\", classification_report(y_test, lstm_preds, target_names=le.classes_))\n",
    "print(\"\\nClassification Report (Meta Model):\\n\", classification_report(y_test, meta_preds, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58baf1bc",
   "metadata": {},
   "source": [
    "Saving the meta_model model.pkl file for deployment step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc633527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the path one directory back and then into 'models'\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "models_dir = os.path.join(base_dir, 'models')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save Scikit-learn models\n",
    "joblib.dump(grid_rf.best_estimator_, os.path.join(models_dir, \"random_forest_model.pkl\"))\n",
    "joblib.dump(meta_model, os.path.join(models_dir, \"meta_model.pkl\"))\n",
    "joblib.dump(scaler, os.path.join(models_dir, \"scaler.pkl\"))\n",
    "joblib.dump(le, os.path.join(models_dir, \"label_encoder.pkl\"))\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_model.save(os.path.join(models_dir, \"lstm_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crypto_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
